import chromadb
from sentence_transformers import SentenceTransformer
import requests
import json
import re
import os

# è¨­å®šå¯é…ç½®çš„ top_k å€¼
DEFAULT_TOP_K = int(os.environ.get("RAG_TOP_K", 7))

# è¼‰å…¥æ‰€æœ‰å•†å“è³‡æ–™ä»¥ä¾›ç›´æ¥æŸ¥è©¢
ALL_ITEMS_PATH = "shopee_processed_results-all.json"
with open(ALL_ITEMS_PATH, "r", encoding="utf-8") as f:
    all_items = json.load(f)

# åˆå§‹åŒ– ChromaDB
client = chromadb.PersistentClient(path="chroma_store")
collection = client.get_collection(name="coffee")

# æŠ“å‡ºæ‰€æœ‰å·²åŠ å…¥çš„ documents + metadatas
all_data = collection.get(include=["documents", "metadatas"])

# åªå°å‰ 5 ç­†ç¤ºç¯„
for i, (doc, meta) in enumerate(zip(all_data["documents"], all_data["metadatas"])):
    print(f"\n--- Item {i} ---")
    print("DOCUMENT (ä½ å­˜çš„å…§å®¹):")
    print(doc)
    print("\nMETADATA (ä½ å­˜çš„æ¬„ä½):")
    print(meta)
    if i >= 4:
        break

# è¼‰å…¥ embedder
embedder = SentenceTransformer("BAAI/bge-small-zh-v1.5")

# Ollama API è¨­å®š
OLLAMA_API_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "qwen2.5:0.5b"  # è«‹å¡«å…¥ä½ çš„æ¨¡å‹åç¨±

# é—œéµå­—æå–å‡½å¼
def extract_keywords(text):
    words = re.findall(r"[\u4e00-\u9fa5a-zA-Z0-9]+", text)
    return [w.lower() for w in words if len(w) >= 1]

# ç›´æ¥æ ¹æ“šå­—ä¸²åœ¨å…¨éƒ¨è³‡æ–™åšå­å­—ä¸²æ¯”å°
def direct_search(query):
    q = query.lower()
    results = []
    for item in all_items:
        name = (item.get("name") or "").lower()
        desc = (item.get("description") or "").lower()
        if q in name or q in desc:
            results.append({
                "name": item.get("name"),
                "price": item.get("price"),
                "sold_count": item.get("sold_count"),
                "rating": item.get("rating"),
                "description": item.get("description"),
                "link": item.get("link")
            })
    return results

# å…ˆåšèªç¾©æª¢ç´¢ï¼Œå†è£œä¸Šç›´æ¥æª¢ç´¢çµæœ
# ä¿®æ”¹ semantic_search å‡½æ•¸ä»¥ä½¿ç”¨ DEFAULT_TOP_K ä¸¦åŒ…å« document content
def semantic_search(query, n=DEFAULT_TOP_K): # Use DEFAULT_TOP_K as default for n
    emb = embedder.encode([query])[0]
    resp = collection.query(
        query_embeddings=[emb.tolist()],
        n_results=n, # n will take the value of DEFAULT_TOP_K from function signature
        include=["metadatas", "documents"] # Include document content
    )

    processed_results = []
    docs = resp.get("documents", [[]])[0]
    metas = resp.get("metadatas", [[]])[0]
    ids = resp.get("ids", [[]])[0] # Assuming ids are always returned and align

    for i in range(len(ids)):
        doc_content = docs[i] if i < len(docs) else None
        meta_content = metas[i] if i < len(metas) else None
        if doc_content and meta_content:
            processed_results.append({'document_content': doc_content, 'metadata': meta_content})
    return processed_results

# å®šç¾©æ–°çš„ LLM æç¤ºæ¨¡æ¿
new_prompt_template = '''
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å’–å•¡ç”¢å“é¡§å•ã€‚è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„å¤šå€‹ã€Œè³‡æ–™ç‰‡æ®µã€ä¾†å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚
æ¯å€‹ç‰‡æ®µå¯èƒ½åªåŒ…å«ç”¢å“çš„éƒ¨åˆ†è³‡è¨Šï¼ˆä¾‹å¦‚ï¼Œæ ¸å¿ƒè³‡è¨Šã€æè¿°çš„ä¸€éƒ¨åˆ†ã€å–®ä¸€å±¬æ€§ç­‰ï¼‰ã€‚
ä½ éœ€è¦ç¶œåˆåˆ¤æ–·é€™äº›ç‰‡æ®µï¼Œç‰¹åˆ¥æ³¨æ„å…·æœ‰ç›¸åŒã€Œç”¢å“ID (`doc_id`)ã€çš„ç‰‡æ®µé€šå¸¸å±¬æ–¼åŒä¸€å€‹ç”¢å“ã€‚

ã€è³‡æ–™ç‰‡æ®µé–‹å§‹ã€‘
{context}
ã€è³‡æ–™ç‰‡æ®µçµæŸã€‘

ç”¨æˆ¶çš„å•é¡Œæ˜¯ï¼šã€Œ{query}ã€

è«‹ä¾ç…§ä»¥ä¸‹æŒ‡ç¤ºä½œç­”ï¼š
1.  **æ•´åˆè³‡è¨Š**ï¼šå¦‚æœå¤šå€‹è³‡æ–™ç‰‡æ®µçœ‹èµ·ä¾†æè¿°åŒä¸€å€‹ç”¢å“ï¼ˆåŸºæ–¼å…§å®¹æˆ–å…ƒæ•¸æ“šä¸­çš„ `doc_id`ï¼‰ï¼Œè«‹æ•´åˆé€™äº›è³‡è¨Šä¾†å½¢æˆå°è©²ç”¢å“çš„æ›´å®Œæ•´ç†è§£ã€‚
2.  **å›ç­”å•é¡Œ**ï¼šç›´æ¥å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚
3.  **ç”¢å“æ¨è–¦ (è‹¥é©ç”¨)**ï¼šå¦‚æœå•é¡Œæ¶‰åŠå°‹æ‰¾ç”¢å“ï¼Œè«‹æ¨è–¦1è‡³3å€‹æœ€ç›¸é—œçš„ç”¢å“ã€‚å°æ–¼æ¯å€‹æ¨è–¦çš„ç”¢å“ï¼Œè«‹æä¾›ï¼š
    *   å•†å“åç¨± (å¯å¾ç‰‡æ®µå…§å®¹æˆ–å…ƒæ•¸æ“š `name` ç²å¾—)
    *   åƒ¹æ ¼ (å¯å¾ç‰‡æ®µå…§å®¹æˆ–å…ƒæ•¸æ“š `price` ç²å¾—)
    *   æè¿°æ‘˜è¦ (å¾ç›¸é—œç‰‡æ®µçš„ `content` å’Œå…ƒæ•¸æ“šä¸­çš„ `description` ç¶œåˆ)
    *   å•†åº—åç¨± (å¯å¾ç‰‡æ®µå…§å®¹æˆ–å…ƒæ•¸æ“š `shop_name` ç²å¾—)
    *   (å¯é¸) ä»»ä½•èˆ‡æŸ¥è©¢ç›¸é—œçš„é¡¯è‘—ç‰¹é»æˆ–å±¬æ€§ã€‚
4.  **æœ€ä½åƒ¹ (è‹¥é©ç”¨)**ï¼šå¦‚æœæŸ¥è©¢è¦æ±‚æˆ–ä¸Šä¸‹æ–‡ä¸­æœ‰å¤šå€‹åƒ¹æ ¼ï¼Œè«‹æŒ‡å‡ºæ‰¾åˆ°çš„æœ€ä½åƒ¹æ ¼çš„å•†å“ã€‚
5.  **ä¾æ“šè³‡æ–™**ï¼šåš´æ ¼æ ¹æ“šæä¾›çš„è³‡æ–™ç‰‡æ®µä½œç­”ã€‚ä¸è¦ç·¨é€ è³‡æ–™ä»¥å¤–çš„è³‡è¨Šã€‚
6.  **ç„¡ç›¸é—œè³‡è¨Š**ï¼šå¦‚æœè³‡æ–™ç‰‡æ®µä¸­ç¢ºå¯¦æ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šä¾†å›ç­”å•é¡Œï¼Œè«‹æ˜ç¢ºèªªæ˜ã€Œæ ¹æ“šæä¾›çš„è³‡æ–™ï¼Œæ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šã€ã€‚
7.  **èªè¨€**ï¼šè«‹ä½¿ç”¨å°ç£æ­£é«”ä¸­æ–‡å›ç­”ã€‚

è«‹ç”Ÿæˆæ‚¨çš„åˆ†æèˆ‡å›ç­”ã€‚
'''

print("è«‹è¼¸å…¥ä½ çš„å•é¡Œï¼ˆæˆ–è¼¸å…¥ exit é›¢é–‹ï¼‰ï¼š")
while True:
    query = input().strip()
    if query.lower() == "exit":
        break

    # å–å¾—èªç¾©æª¢ç´¢èˆ‡ç›´æ¥æª¢ç´¢çµæœ
    # semantic_search will now use DEFAULT_TOP_K by default and return new structure
    sem_results = semantic_search(query)
    direct_results = direct_search(query) # direct_results remain old structure

    # åˆä½µå»é‡ - needs adjustment for new sem_results structure
    # For simplicity, direct search results will be added if their names aren't in semantic results' metadata names
    # A more robust deduplication might involve checking doc_id if direct_results also had it
    seen_names_in_semantic = set()
    if sem_results: # Ensure sem_results is not None and not empty
        for res in sem_results:
            if 'metadata' in res and 'name' in res['metadata']:
                 seen_names_in_semantic.add(res['metadata']['name'])

    merged = sem_results + [d for d in direct_results if d.get('name') not in seen_names_in_semantic]


    if not merged:
        print("âŒ å®Œå…¨æ²’æœ‰æ‰¾åˆ°ç›¸é—œè³‡æ–™ï¼Œè«‹æ›å€‹å•æ³•ï¼")
        continue

    # åˆ—å°æª¢ç´¢åˆ°çš„è³‡æ–™ ( angepasst an die neue Struktur )
    print("\n==== ğŸ“ å–å¾—ç›¸é—œè³‡æ–™ ====")
    for i, item_data in enumerate(merged, start=1):
        if 'document_content' in item_data: # Semantic result
            metadata = item_data['metadata']
            print(f"ã€{i}ã€‘(èªç¾©) {metadata.get('name', 'N/A')} - {metadata.get('price', 'N/A')}å…ƒ (å€å¡Šé¡å‹: {metadata.get('type', 'N/A')})")
            print(f"   å…§å®¹ç‰‡æ®µ: {item_data['document_content'][:100]}...")
        else: # Direct result
            print(f"ã€{i}ã€‘(ç›´æ¥) {item_data.get('name', 'N/A')} - {item_data.get('price', 'N/A')}å…ƒ ({item_data.get('sold_count', 'N/A')}ä»¶å·²å”®)")


    # æº–å‚™ä¸Šä¸‹æ–‡çµ¦ LLM ( angepasst an die neue Struktur )
    context_items = []
    for i, item_data in enumerate(merged):
        context_item = ""
        if 'document_content' in item_data: # Semantic result
            metadata = item_data['metadata']
            chunk_content = item_data['document_content']
            context_item = (
                f"ç‰‡æ®µ {i+1} (ä¾†æº: èªç¾©æª¢ç´¢, "
                f"ç”¢å“ID: {metadata.get('doc_id', 'N/A')}, "
                f"å€å¡ŠID: {metadata.get('chunk_id', 'N/A')}, "
                f"å€å¡Šé¡å‹: {metadata.get('type', 'N/A')}, "
                f"åŸå§‹ç”¢å“å: {metadata.get('name', 'N/A')}, "
                f"åŸå§‹ç”¢å“åƒ¹æ ¼: {metadata.get('price', 'N/A')}å…ƒ):\n"
                f"å…§å®¹: {chunk_content}"
            )
        else: # Direct result
            chunk_content = (
                f"å•†å“åç¨±ï¼š{item_data.get('name', '')}\n"
                f"åƒ¹æ ¼ï¼š{item_data.get('price', '')}å…ƒ\n"
                f"æè¿°ï¼š{item_data.get('description', '')}"
            )
            context_item = (
                f"ç‰‡æ®µ {i+1} (ä¾†æº: ç›´æ¥æœå°‹, "
                f"ç”¢å“å: {item_data.get('name', 'N/A')}, "
                f"åƒ¹æ ¼: {item_data.get('price', 'N/A')}å…ƒ):\n"
                f"å…§å®¹: {chunk_content}"
            )
        context_items.append(context_item)
    context_string = "\n\n".join(context_items)

    prompt = new_prompt_template.format(context=context_string, query=query)

    payload = {"model": OLLAMA_MODEL, "prompt": prompt, "stream": False}
    response = requests.post(OLLAMA_API_URL, json=payload)

    if response.status_code == 200:
        data = response.json()
        print("\n==== ğŸ¤– å›ç­” ====")
        print(data.get("response", "âš ï¸ ç„¡å›æ‡‰å…§å®¹"))
    else:
        print(f"âš ï¸ LLM å‘¼å«å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")
