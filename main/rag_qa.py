import chromadb
from sentence_transformers import SentenceTransformer
import requests
import json
import re
import os

# è¼‰å…¥æ‰€æœ‰å•†å“è³‡æ–™ä»¥ä¾›ç›´æ¥æŸ¥è©¢
ALL_ITEMS_PATH = "shopee_processed_results-all.json"
with open(ALL_ITEMS_PATH, "r", encoding="utf-8") as f:
    all_items = json.load(f)

# åˆå§‹åŒ– ChromaDB
client = chromadb.PersistentClient(path="chroma_store")
collection = client.get_collection(name="coffee")

# æŠ“å‡ºæ‰€æœ‰å·²åŠ å…¥çš„ documents + metadatas
all_data = collection.get(include=["documents", "metadatas"])

# åªå°å‰ 5 ç­†ç¤ºç¯„
for i, (doc, meta) in enumerate(zip(all_data["documents"], all_data["metadatas"])):
    print(f"\n--- Item {i} ---")
    print("DOCUMENT (ä½ å­˜çš„å…§å®¹):")
    print(doc)
    print("\nMETADATA (ä½ å­˜çš„æ¬„ä½):")
    print(meta)
    if i >= 4:
        break

# è¼‰å…¥ embedder
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Ollama API è¨­å®š
OLLAMA_API_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "qwen2.5:0.5b"  # è«‹å¡«å…¥ä½ çš„æ¨¡å‹åç¨±

# é—œéµå­—æå–å‡½å¼
def extract_keywords(text):
    words = re.findall(r"[\u4e00-\u9fa5a-zA-Z0-9]+", text)
    return [w.lower() for w in words if len(w) >= 1]

# ç›´æ¥æ ¹æ“šå­—ä¸²åœ¨å…¨éƒ¨è³‡æ–™åšå­å­—ä¸²æ¯”å°
def direct_search(query):
    q = query.lower()
    results = []
    for item in all_items:
        name = (item.get("name") or "").lower()
        desc = (item.get("description") or "").lower()
        if q in name or q in desc:
            results.append({
                "name": item.get("name"),
                "price": item.get("price"),
                "sold_count": item.get("sold_count"),
                "rating": item.get("rating"),
                "description": item.get("description"),
                "link": item.get("link")
            })
    return results

# å…ˆåšèªç¾©æª¢ç´¢ï¼Œå†è£œä¸Šç›´æ¥æª¢ç´¢çµæœ
def semantic_search(query, n=20):
    emb = embedder.encode([query])[0]
    resp = collection.query(
        query_embeddings=[emb.tolist()],
        n_results=n,
        include=["metadatas"]
    )
    return [m for m in resp.get("metadatas", [[]])[0] if m]

print("è«‹è¼¸å…¥ä½ çš„å•é¡Œï¼ˆæˆ–è¼¸å…¥ exit é›¢é–‹ï¼‰ï¼š")
while True:
    query = input().strip()
    if query.lower() == "exit":
        break

    # å–å¾—èªç¾©æª¢ç´¢èˆ‡ç›´æ¥æª¢ç´¢çµæœ
    sem_results = semantic_search(query, n=15)
    direct_results = direct_search(query)

    # åˆä½µå»é‡
    seen = set([m['name'] for m in sem_results])
    merged = sem_results + [d for d in direct_results if d['name'] not in seen]

    if not merged:
        print("âŒ å®Œå…¨æ²’æœ‰æ‰¾åˆ°ç›¸é—œè³‡æ–™ï¼Œè«‹æ›å€‹å•æ³•ï¼")
        continue

    # åˆ—å°æª¢ç´¢åˆ°çš„è³‡æ–™
    print("\n==== ğŸ“ å–å¾—ç›¸é—œè³‡æ–™ ====")
    for i, m in enumerate(merged, start=1):
        print(f"ã€{i}ã€‘{m['name']} - {m['price']}å…ƒ ({m['sold_count']}ä»¶å·²å”®)")

    # æº–å‚™ä¸Šä¸‹æ–‡çµ¦ LLM
    context = "\n\n".join(
        f"å•†å“åç¨±ï¼š{m['name']}\nåƒ¹æ ¼ï¼š{m['price']}å…ƒ\næè¿°ï¼š{m['description']}"
        for m in merged
    )
    prompt = f"""
ä½ æ˜¯ä¸€ä½å’–å•¡ç”¢å“åˆ†æå°ˆå®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚

ã€è³‡æ–™é–‹å§‹ã€‘
{context}
ã€è³‡æ–™çµæŸã€‘

ç”¨æˆ¶çš„å•é¡Œæ˜¯ï¼šã€Œ{query}ã€

å›ç­”è¦æ±‚ï¼š
- åƒ…èƒ½æ ¹æ“šä¸Šè¿°è³‡æ–™ä½œç­”ã€‚
- è‹¥è³‡æ–™ä¸­æ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šï¼Œè«‹ç›´æ¥å›ç­”ã€Œè³‡æ–™ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šã€ã€‚
- è‹¥æœ‰ç¬¦åˆçš„å•†å“ï¼Œè«‹åˆ—å‡ºå•†å“åç¨±èˆ‡åƒ¹æ ¼ï¼Œä¸¦æŒ‡å‡ºæœ€ä½åƒ¹ã€‚

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚
"""

    payload = {"model": OLLAMA_MODEL, "prompt": prompt, "stream": False}
    response = requests.post(OLLAMA_API_URL, json=payload)

    if response.status_code == 200:
        data = response.json()
        print("\n==== ğŸ¤– å›ç­” ====")
        print(data.get("response", "âš ï¸ ç„¡å›æ‡‰å…§å®¹"))
    else:
        print(f"âš ï¸ LLM å‘¼å«å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")
